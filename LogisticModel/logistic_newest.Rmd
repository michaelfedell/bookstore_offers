---
title: "logistic_newest"
author: "Eileen Zhang"
date: "12/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('./loader.R')
```

```{r}
library(car)
```

```{r}
train.logistic <- train.full %>% 
  select(id, avgNetOrder, sumQty, logtargamt, sumQtyPerTof,
         recency, frequency, amount, tof, respond, returned,
         oneMonth, threeMonth, sixMonth, oneYear, overYear, amountPer, maxNet) %>%
  filter(tof > 0)

train.logistic.res = train.logistic[train.logistic$respond ==1,]
train.logistic.fold = train.logistic.res[rep(seq(nrow(train.logistic.res)), 3),]
train.rebalanced = rbind(train.logistic.fold,train.logistic)
length(which(train.rebalanced$respond == 1))

#fit a baseline model using all predictors
logistic.all <- glm(respond ~ avgNetOrder + sumQty + recency + frequency  + tof + amount + amountPer +
                      oneMonth + threeMonth + sixMonth + oneYear + overYear + sumQtyPerTof + returned + maxNet,
                    data = train.rebalanced, family = binomial)
#Remove outliers based on cooks distance
train.rebalanced <- train.rebalanced %>% 
  filter(cooks.distance(logistic.all) < qf(0.1,15,9049))

#check multicollinearity
vif(logistic.all)

```

removed sumQty since vif>10

```{r}
logistic.1 = glm(respond ~ avgNetOrder + recency + frequency  + tof + amount + amountPer +
                      oneMonth + threeMonth + sixMonth + oneYear + overYear + sumQtyPerTof + returned + maxNet,
                    data = train.rebalanced, family = binomial)

step(logistic.1, scope = ~ avgNetOrder + recency + frequency  + tof + amount + amountPer +
                      oneMonth + threeMonth + sixMonth + oneYear + overYear + sumQtyPerTof + returned + maxNet,
     direction = "both")
```

```{r}
#stepwise selected
logistic.2 = glm(respond ~ recency + frequency  + tof + amount + amountPer +
                      oneMonth + threeMonth + sixMonth + overYear + sumQtyPerTof + maxNet,
                    data = train.rebalanced, family = binomial)
summary(logistic.2)
```
```{r}
#remove amountPer and sixmonth
log.best = glm(respond ~ recency + frequency  + tof + amount +
                      oneMonth + threeMonth + overYear + maxNet +sumQtyPerTof,
                    data = train.rebalanced, family = binomial)
summary(log.best)
```

#check agin for vif
```{r}
vif(log.best)
```

No apparent mutlicollinearity problems

Note: The fitted model is trained on ovesampled data and thus biased slightly towards successes (logtargamt > 0). This will need to be corrected for later when testing against the model's predictions.

## Validation

Raw predictions from model will be biased towards responders because of the oversampling step.
This can be corrected for by the following formula
$ln[p1 / (1 - p1)] = -ln(m)+ln[p2 / (1 - p2)]$
=> $p1 = exp(-ln(m) + ln(p2 / (1 - p2)))/[1 + exp(-ln(m) + ln(p2 / (1 - p2)))]$
Where $m$ is the number of folds to oversample data,  
$p1$ is the probabilities for the raw data with no oversampling, and  
$p2$ is the probabilities for oversampled data (model output)

```{r}
p2 <- predict(log.best, newdata = test.full, type = 'response')
#m <- nrow(filter(train.rebalanced, respond == 1)) / nrow(train.rebalanced) / .0393
p1 <- exp(-log(4) + log(p2 / (1 - p2)))/(1 + exp(-log(4) + log(p2 / (1 - p2))))
```

p1 now contains the predicted probability of responding to the offer after correcting for oversampling. These probabilities can be used to make "hard" classifications which will allow direct evaluation of the classifier.

```{r}
library(pROC)
classifications <- p1 > 0.1
ground.truth <- test.full$logtargamt > 0

conf.matrix <- table(classifications, ground.truth)
conf.matrix
pROC::plot.roc(test.full$logtargamt > 0, p1, xlab = "Spec", ylab = "Sens", print.auc = T)

rocobj <- roc(test.full$logtargamt > 0 , p1)
coords(rocobj, "best")

p.star <- 0.5
precision <- conf.matrix[2,2] / sum(conf.matrix[, 2]); precision
recall <- conf.matrix[2,2] / sum(conf.matrix[2, ]); recall
F1 <- (2 * precision * recall) / (precision + recall); F1
```
